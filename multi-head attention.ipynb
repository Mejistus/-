{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada54ea548ea224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd1b5a78e6a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./self_attention.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.WQ = nn.Parameter(torch.randn(input_size, input_size))\n",
    "        self.WK = nn.Parameter(torch.randn(input_size, input_size))\n",
    "        self.WV = nn.Parameter(torch.randn(input_size, input_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 计算注意力权重\n",
    "        scores = torch.bmm(x, self.W.unsqueeze(0))\n",
    "        attention_weights = torch.softmax(scores, dim=1)\n",
    "        # 计算注意力输出\n",
    "        output = torch.bmm(attention_weights, x)\n",
    "        return output\n",
    "\n",
    "# 测试代码\n",
    "if __name__ == '__main__':\n",
    "    input_tensor = torch.randn(3, 8)\n",
    "    attention = SelfAttention(8)\n",
    "    output = attention(input_tensor)\n",
    "    print(output.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd96040e58f9a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / torch.sqrt(self.v.size(0))\n",
    "        self.v.data.normal_(mean=0, std=stdv)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        attn_weights = F.softmax(self.attn(output[0]).mm(self.v), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), output).squeeze(0)\n",
    "        output = self.fc(attn_applied)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(self.num_layers, 1, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, 1, self.hidden_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b59ed067e0e7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
